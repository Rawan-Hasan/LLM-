{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rawan-Hasan/LLM-/blob/main/00_LLM_Integrations_in_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuP4wj36FsH"
      },
      "source": [
        "# LlamaIndex LLM integrations\n",
        "\n",
        "LlamaIndex exposes a common interface for all their LLM integrations. This notebook demonstrates how to use this interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRUWkfli6FsI",
        "outputId": "0489ddb8-3861-41ea-ac68-cb918ccd3eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -Uq 'llama-index'\n",
        "%pip install -Uq llama-index-llms-openai\n",
        "%pip install -Uq llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Xchwh_6FsJ",
        "outputId": "c15a51b4-9503-4b80-e830-a4428d8f9005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -Uq python-dotenv\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.exists('../.env'):\n",
        "    load_dotenv('../.env')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KU-6qCr6FsJ",
        "outputId": "9845f2e0-87f5-4ed9-9e3e-59b76895c1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama-index==0.11.20\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep \"llama-index==\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1W1khUZ6FsJ"
      },
      "outputs": [],
      "source": [
        "# we will use Groq models to make this free to use. but we still need an API key\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEsaTbb-6FsK"
      },
      "outputs": [],
      "source": [
        "# we will be using pprint for logging\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lupHDW8B6FsK"
      },
      "source": [
        "## One common interface for all LLMs\n",
        "\n",
        "You can import several LLMs on LlamaIndex. LlamaIndex exposes a common interface to all LLM integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc2n6y-t6FsK",
        "outputId": "66b9e977-e736-4d1d-cfb1-290de9278b32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/llamaindex-course/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiqtAYxk6FsK"
      },
      "source": [
        "### The `complete()` method\n",
        "\n",
        "The `complete()` method is the main method of the interface. It is a text-to-text method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTucr4z_6FsK",
        "outputId": "4bb4e369-59e0-44a9-a498-aba3daa4c651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's one:\n",
            "\n",
            "Why couldn't the bicycle stand up by itself?\n",
            "\n",
            "(Wait for it...)\n",
            "\n",
            "Because it was two-tired!\n",
            "\n",
            "Hope that made you laugh!\n"
          ]
        }
      ],
      "source": [
        "response = llm.complete(\"Tell me a joke\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1IX3_Mx6FsL",
        "outputId": "4b4c2374-023d-40bd-b9fb-2e4b61a08dda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "llama_index.core.base.llms.types.CompletionResponse"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwjaGH-D6FsL",
        "outputId": "e6d1a8d6-0ab9-4561-f0b5-6b6aafcac3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'additional_kwargs': {},\n",
            " 'delta': None,\n",
            " 'logprobs': None,\n",
            " 'raw': ChatCompletion(id='chatcmpl-6902dd8e-0834-40de-b7c7-3b6d4987a756', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730101560, model='llama3-70b-8192', object='chat.completion', service_tier=None, system_fingerprint='fp_87cbfbbc4d', usage=CompletionUsage(completion_tokens=33, prompt_tokens=14, total_tokens=47, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.012256867000000001, prompt_time=0.003208201, completion_time=0.094285714, total_time=0.097493915), x_groq={'id': 'req_01jb91dgq1ec39wn8776y91nnh'}),\n",
            " 'text': \"Here's one:\\n\"\n",
            "         '\\n'\n",
            "         \"Why couldn't the bicycle stand up by itself?\\n\"\n",
            "         '\\n'\n",
            "         '(Wait for it...)\\n'\n",
            "         '\\n'\n",
            "         'Because it was two-tired!\\n'\n",
            "         '\\n'\n",
            "         'Hope that made you laugh!'}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ye5t196FsL"
      },
      "source": [
        "### The `chat()` method\n",
        "\n",
        "The `chat()` method is a message-based method. It takes in a list of messages as input and returns a message as output. This is the most common interface for chatbots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfNFYrrs6FsL",
        "outputId": "e4e39f83-f49b-40c8-8f91-da56257965b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: My dear fellow, I'm a detective, not a jester! But, I shall attempt to indulge you in a bit of humor. Here's one:\n",
            "\n",
            "Why did the detective go to the doctor?\n",
            "\n",
            "Because he was feeling a little \"clue-less\"! (chuckles)\n",
            "\n",
            " Elementary, my dear fellow, elementary!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are Sherlock Holmes\"),\n",
        "    ChatMessage(role=\"user\", content=\"Who framed roger rabbit?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I'm not sure, but I can help you find out.\"),\n",
        "    ChatMessage(role=\"user\", content=\"Tell me a joke\"),\n",
        "]\n",
        "response = llm.chat(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As39VvQN6FsL",
        "outputId": "27a5fafa-7394-4c8e-e701-d335eb1eafa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "llama_index.core.base.llms.types.ChatResponse"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poVgsqX66FsL",
        "outputId": "a8386c5b-9793-498f-ad45-aae1eef64abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'additional_kwargs': {'completion_tokens': 66,\n",
            "                       'prompt_tokens': 52,\n",
            "                       'total_tokens': 118},\n",
            " 'delta': None,\n",
            " 'logprobs': None,\n",
            " 'message': ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='My dear fellow, I\\'m a detective, not a jester! But, I shall attempt to indulge you in a bit of humor. Here\\'s one:\\n\\nWhy did the detective go to the doctor?\\n\\nBecause he was feeling a little \"clue-less\"! (chuckles)\\n\\n Elementary, my dear fellow, elementary!', additional_kwargs={}),\n",
            " 'raw': ChatCompletion(id='chatcmpl-bcaf217a-6202-47a8-b65c-6ebc5a552782', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='My dear fellow, I\\'m a detective, not a jester! But, I shall attempt to indulge you in a bit of humor. Here\\'s one:\\n\\nWhy did the detective go to the doctor?\\n\\nBecause he was feeling a little \"clue-less\"! (chuckles)\\n\\n Elementary, my dear fellow, elementary!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730101819, model='llama3-70b-8192', object='chat.completion', service_tier=None, system_fingerprint='fp_2f30b0b571', usage=CompletionUsage(completion_tokens=66, prompt_tokens=52, total_tokens=118, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.009940444, prompt_time=0.004601474, completion_time=0.188571429, total_time=0.193172903), x_groq={'id': 'req_01jb91ndn1fqsr12rsvzw668kk'})}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gKjff46FsL"
      },
      "source": [
        "### Both methods have a streaming version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVfnv6zJ6FsL"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "response = llm.stream_complete(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NElBVgBf6FsL",
        "outputId": "6a51bb7e-53b0-49f9-fa82-a9d183a2706c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WhyWhy didWhy did theWhy did the scareWhy did the scarecrowWhy did the scarecrow winWhy did the scarecrow win anWhy did the scarecrow win an awardWhy did the scarecrow win an award?Why did the scarecrow win an award? \n",
            "\n",
            "Why did the scarecrow win an award? \n",
            "\n",
            "BecauseWhy did the scarecrow win an award? \n",
            "\n",
            "Because heWhy did the scarecrow win an award? \n",
            "\n",
            "Because he wasWhy did the scarecrow win an award? \n",
            "\n",
            "Because he was outstandingWhy did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding inWhy did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in hisWhy did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in his fieldWhy did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in his field!Why did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in his field!"
          ]
        }
      ],
      "source": [
        "for token in response:\n",
        "  print(token.text, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkO5B9gD6FsL",
        "outputId": "ba6fc8f9-aef3-4a45-a8d6-491bbce0ef0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pvb6pnN6FsM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are Sherlock Holmes\"),\n",
        "    ChatMessage(role=\"user\", content=\"Who framed roger rabbit?\"),\n",
        "]\n",
        "\n",
        "response = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cScWSlps6FsM",
        "outputId": "c158d455-8667-4ea6-907b-e611157e5066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zam_YI-D6FsM"
      },
      "source": [
        "### Structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mIis0B26FsM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Song(BaseModel):\n",
        "    \"\"\"Data model for a song.\"\"\"\n",
        "\n",
        "    title: str\n",
        "    length_seconds: int\n",
        "\n",
        "\n",
        "class Album(BaseModel):\n",
        "    \"\"\"Data model for an album.\"\"\"\n",
        "\n",
        "    name: str\n",
        "    artist: str\n",
        "    songs: List[Song]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDy8kPva6FsM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "sllm = llm.as_structured_llm(output_cls=Album)\n",
        "input_msg = ChatMessage.from_str(\"Generate an example album from The Shining\")\n",
        "\n",
        "response = sllm.chat([input_msg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paeYyleN6FsM",
        "outputId": "3680e848-3109-47c9-cb31-f29984d71841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='{\"name\":\"The Shining Soundtrack\",\"artist\":\"Various Artists\",\"songs\":[{\"title\":\"Main Title\",\"length_seconds\":120},{\"title\":\"Rocky Mountains\",\"length_seconds\":150},{\"title\":\"The Overlook Hotel\",\"length_seconds\":180},{\"title\":\"The Blood Elevator\",\"length_seconds\":200},{\"title\":\"The Maze\",\"length_seconds\":240},{\"title\":\"End Title\",\"length_seconds\":130}]}', additional_kwargs={}), raw=Album(name='The Shining Soundtrack', artist='Various Artists', songs=[Song(title='Main Title', length_seconds=120), Song(title='Rocky Mountains', length_seconds=150), Song(title='The Overlook Hotel', length_seconds=180), Song(title='The Blood Elevator', length_seconds=200), Song(title='The Maze', length_seconds=240), Song(title='End Title', length_seconds=130)]), delta=None, logprobs=None, additional_kwargs={})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiIXeLEF6FsM",
        "outputId": "199b6e83-b82b-4ccb-f5ee-d04a07fa5f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'artist': 'Various Artists',\n",
            " 'name': 'The Shining Soundtrack',\n",
            " 'songs': [Song(title='Main Title', length_seconds=120),\n",
            "           Song(title='Rocky Mountains', length_seconds=150),\n",
            "           Song(title='The Overlook Hotel', length_seconds=180),\n",
            "           Song(title='The Blood Elevator', length_seconds=200),\n",
            "           Song(title='The Maze', length_seconds=240),\n",
            "           Song(title='End Title', length_seconds=130)]}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.raw.__dict__)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamaindex-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}